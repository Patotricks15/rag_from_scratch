{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Fazer o embedding usando esse\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(text), chunk_size-overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    response = embedder.encode(text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "        filter_func (callable, optional): Function to filter results.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Apply filter if provided\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for use with adaptive retrieval.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for the text chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize the vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its embedding to the vector store with metadata\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    \n",
    "    # Return the chunks and the vector store\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_if_retrieval_needed(query):\n",
    "    \"\"\"\n",
    "    Determines if retrieval is necessary for the given query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if retrieval is needed, False otherwise\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to determine if retrieval is necessary\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if retrieval is necessary to answer a query.\n",
    "    For factual questions, specific information requests, or questions about events, people, or concepts, answer \"Yes\".\n",
    "    For opinions, hypothetical scenarios, or simple queries with common knowledge, answer \"No\".\n",
    "    Answer with ONLY \"Yes\" or \"No\".\"\"\"\n",
    "\n",
    "    # User prompt containing the query\n",
    "    user_prompt = f\"Query: {query}\\n\\nIs retrieval necessary to answer this query accurately?\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    # Return True if the answer contains \"yes\", otherwise return False\n",
    "    return \"yes\" in answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(query, context):\n",
    "    \"\"\"\n",
    "    Evaluates the relevance of a context to the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: 'relevant' or 'irrelevant'\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to determine document relevance\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if a document is relevant to a query.\n",
    "    Consider whether the document contains information that would be helpful in answering the query.\n",
    "    Answer with ONLY \"Relevant\" or \"Irrelevant\".\"\"\"\n",
    "\n",
    "    # Truncate context if it is too long to avoid exceeding token limits\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "\n",
    "    # User prompt containing the query and the document content\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Document content:\n",
    "    {context}\n",
    "\n",
    "    Is this document relevant to the query? Answer with ONLY \"Relevant\" or \"Irrelevant\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # Return the relevance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_support(response, context):\n",
    "    \"\"\"\n",
    "    Assesses how well a response is supported by the context.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Generated response\n",
    "        context (str): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: 'fully supported', 'partially supported', or 'no support'\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to evaluate support\n",
    "    system_prompt = \"\"\"You are an AI assistant that determines if a response is supported by the given context.\n",
    "    Evaluate if the facts, claims, and information in the response are backed by the context.\n",
    "    Answer with ONLY one of these three options:\n",
    "    - \"Fully supported\": All information in the response is directly supported by the context.\n",
    "    - \"Partially supported\": Some information in the response is supported by the context, but some is not.\n",
    "    - \"No support\": The response contains significant information not found in or contradicting the context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate context if it is too long to avoid exceeding token limits\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "\n",
    "    # User prompt containing the context and the response to be evaluated\n",
    "    user_prompt = f\"\"\"Context:\n",
    "    {context}\n",
    "\n",
    "    Response:\n",
    "    {response}\n",
    "\n",
    "    How well is this response supported by the context? Answer with ONLY \"Fully supported\", \"Partially supported\", or \"No support\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the answer from the model's response and convert to lowercase\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    \n",
    "    return answer  # Return the support assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_utility(query, response):\n",
    "    \"\"\"\n",
    "    Rates the utility of a response for the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        response (str): Generated response\n",
    "        \n",
    "    Returns:\n",
    "        int: Utility rating from 1 to 5\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to rate the utility of the response\n",
    "    system_prompt = \"\"\"You are an AI assistant that rates the utility of a response to a query.\n",
    "    Consider how well the response answers the query, its completeness, correctness, and helpfulness.\n",
    "    Rate the utility on a scale from 1 to 5, where:\n",
    "    - 1: Not useful at all\n",
    "    - 2: Slightly useful\n",
    "    - 3: Moderately useful\n",
    "    - 4: Very useful\n",
    "    - 5: Exceptionally useful\n",
    "    Answer with ONLY a single number from 1 to 5.\"\"\"\n",
    "\n",
    "    # User prompt containing the query and the response to be rated\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Response:\n",
    "    {response}\n",
    "\n",
    "    Rate the utility of this response on a scale from 1 to 5:\"\"\"\n",
    "    \n",
    "    # Generate the utility rating using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the rating from the model's response\n",
    "    rating = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract just the number from the rating\n",
    "    rating_match = re.search(r'[1-5]', rating)\n",
    "    if rating_match:\n",
    "        return int(rating_match.group())  # Return the extracted rating as an integer\n",
    "    \n",
    "    return 3  # Default to middle rating if parsing fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"AI_Information.pdf\"\n",
    "\n",
    "# Process the document (extract text, create chunks, generate questions, build vector store)\n",
    "vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search using the query and vector store.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query.\n",
    "    vector_store (SimpleVectorStore): The vector store to search in.\n",
    "    k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: Top k most relevant items.\n",
    "    \"\"\"\n",
    "    # Create embedding for the query\n",
    "    query_embedding_response = create_embeddings(query)\n",
    "    query_embedding = query_embedding_response\n",
    "    \n",
    "    # Search the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context=None):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and optional context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str, optional): Context text\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the AI on how to generate a helpful response\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Provide a clear, accurate, and informative response to the query.\"\"\"\n",
    "    \n",
    "    # Create the user prompt based on whether context is provided\n",
    "    if context:\n",
    "        user_prompt = f\"\"\"Context:\n",
    "        {context}\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Please answer the query based on the provided context.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "        \n",
    "        Please answer the query to the best of your ability.\"\"\"\n",
    "    \n",
    "    # Generate the response using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Return the generated response text\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    Implements the complete Self-RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store containing document chunks\n",
    "        top_k (int): Number of documents to retrieve initially\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results including query, response, and metrics from the Self-RAG process\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Determine if retrieval is necessary\n",
    "    retrieval_needed = determine_if_retrieval_needed(query)\n",
    "    \n",
    "    # Initialize metrics to track the Self-RAG process\n",
    "    metrics = {\n",
    "        \"retrieval_needed\": retrieval_needed,\n",
    "        \"documents_retrieved\": 0,\n",
    "        \"relevant_documents\": 0,\n",
    "        \"response_support_ratings\": [],\n",
    "        \"utility_ratings\": []\n",
    "    }\n",
    "    \n",
    "    best_response = None\n",
    "    best_score = -1\n",
    "    \n",
    "    if retrieval_needed:\n",
    "        # Step 2: Retrieve documents\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        metrics[\"documents_retrieved\"] = len(results)\n",
    "        \n",
    "        # Step 3: Evaluate relevance of each document\n",
    "        relevant_contexts = []\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            context = result[\"text\"]\n",
    "            relevance = evaluate_relevance(query, context)\n",
    "            print(f\"Document {i+1} relevance: {relevance}\")\n",
    "            \n",
    "            if relevance == \"relevant\":\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        metrics[\"relevant_documents\"] = len(relevant_contexts)\n",
    "        \n",
    "        if relevant_contexts:\n",
    "            # Step 4: Process each relevant context\n",
    "            for i, context in enumerate(relevant_contexts):\n",
    "                \n",
    "                # Generate response based on the context\n",
    "                response = generate_response(query, context)\n",
    "                \n",
    "                # Assess how well the response is supported by the context\n",
    "                support_rating = assess_support(response, context)\n",
    "                metrics[\"response_support_ratings\"].append(support_rating)\n",
    "                \n",
    "                # Rate the utility of the response\n",
    "                utility_rating = rate_utility(query, response)\n",
    "                metrics[\"utility_ratings\"].append(utility_rating)\n",
    "                \n",
    "                # Calculate overall score (higher for better support and utility)\n",
    "                support_score = {\n",
    "                    \"fully supported\": 3, \n",
    "                    \"partially supported\": 1, \n",
    "                    \"no support\": 0\n",
    "                }.get(support_rating, 0)\n",
    "                \n",
    "                overall_score = support_score * 5 + utility_rating\n",
    "                print(f\"Overall score: {overall_score}\")\n",
    "                \n",
    "                # Keep track of the best response\n",
    "                if overall_score > best_score:\n",
    "                    best_response = response\n",
    "                    best_score = overall_score\n",
    "                    print(\"New best response found!\")\n",
    "        \n",
    "        # If no relevant contexts were found or all responses scored poorly\n",
    "        if not relevant_contexts or best_score <= 0:\n",
    "            best_response = generate_response(query)\n",
    "    else:\n",
    "        # No retrieval needed, generate directly\n",
    "        best_response = generate_response(query)\n",
    "    \n",
    "    # Final metrics\n",
    "    metrics[\"best_score\"] = best_score\n",
    "    metrics[\"used_retrieval\"] = retrieval_needed and best_score > 0\n",
    "        \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": best_response,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the concerns about AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Document 1 relevance: relevant\n",
      "Document 2 relevance: relevant\n",
      "Document 3 relevance: relevant\n",
      "Overall score: 20\n",
      "New best response found!\n",
      "Overall score: 20\n",
      "Overall score: 20\n"
     ]
    }
   ],
   "source": [
    "chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "# Initialize collection for storing comparison results\n",
    "\n",
    "# Process each test query with both retrieval methods\n",
    "\n",
    "# Create embedding for the query\n",
    "query_embedding = create_embeddings(query)\n",
    "\n",
    "rag_result = self_rag(query, vector_store, top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concerns about AI include:\n",
      "\n",
      "1. Lack of transparency and explainability in decision-making, particularly in deep learning models, leading to difficulty in understanding how decisions are reached.\n",
      "2. Privacy and data security issues due to the reliance of AI systems on large amounts of data, raising concerns about protecting sensitive information and ensuring responsible data handling.\n",
      "3. Job displacement concerns, particularly in industries with repetitive or routine tasks, due to the automation capabilities of AI.\n",
      "4. Questions about control, accountability, and potential unintended consequences as AI systems become more autonomous, highlighting the need for clear guidelines and ethical frameworks for AI development and deployment.\n",
      "5. The potential for the weaponization of AI, raising concerns about the misuse of AI technology for harmful purposes.\n"
     ]
    }
   ],
   "source": [
    "print(rag_result['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
