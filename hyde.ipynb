{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Fazer o embedding usando esse\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file with page separation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of pages with text content and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    pages = []  # Initialize an empty list to store the pages with text content\n",
    "    \n",
    "    # Iterate over each page in the PDF\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]  # Get the current page\n",
    "        text = page.get_text()  # Extract text from the current page\n",
    "        \n",
    "        # Skip pages with very little text (less than 50 characters)\n",
    "        if len(text.strip()) > 50:\n",
    "            # Append the page text and metadata to the list\n",
    "            pages.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_path,  # Source file path\n",
    "                    \"page\": page_num + 1  # Page number (1-based index)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"Extracted {len(pages)} pages with content\")  # Print the number of pages extracted\n",
    "    return pages  # Return the list of pages with text content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunks with metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Iterate over the text in steps of (chunk_size - overlap)\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text = text[i:i + chunk_size]  # Extract the chunk of text\n",
    "        if chunk_text:  # Ensure we don't add empty chunks\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,  # Add the chunk text\n",
    "                \"metadata\": {\n",
    "                    \"start_pos\": i,  # Start position of the chunk in the original text\n",
    "                    \"end_pos\": i + len(chunk_text)  # End position of the chunk in the original text\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of chunks created\n",
    "    return chunks  # Return the list of chunks with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store vector embeddings\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text content\n",
    "            embedding (List[float]): Vector embedding\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Append the embedding as a numpy array\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata or an empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            filter_func (callable, optional): Function to filter results\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return an empty list if there are no vectors\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Skip if doesn't pass the filter\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text content\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": float(score)  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given text.\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s)\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vector(s)\n",
    "    \"\"\"\n",
    "    # Create embeddings for the current batch\n",
    "    response = embedder.encode(text)\n",
    "    return response  # Return the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: Vector store containing document chunks\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Process each page and create chunks\n",
    "    all_chunks = []\n",
    "    for page in pages:\n",
    "        # Pass the text content (string) to chunk_text, not the dictionary\n",
    "        page_chunks = chunk_text(page[\"text\"], chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Update metadata for each chunk with the page's metadata\n",
    "        for chunk in page_chunks:\n",
    "            chunk[\"metadata\"].update(page[\"metadata\"])\n",
    "        \n",
    "        all_chunks.extend(page_chunks)\n",
    "    \n",
    "    # Create embeddings for the text chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in all_chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Create a vector store to hold the chunks and their embeddings\n",
    "    vector_store = SimpleVectorStore()\n",
    "    for i, chunk in enumerate(all_chunks):\n",
    "        vector_store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=chunk_embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"Vector store created with {len(all_chunks)} chunks\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_document(query, desired_length=1000):\n",
    "    \"\"\"\n",
    "    Generate a hypothetical document that answers the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        desired_length (int): Target length of the hypothetical document\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated hypothetical document\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to generate the document\n",
    "    system_prompt = f\"\"\"You are an expert document creator. \n",
    "    Given a question, generate a detailed document that would directly answer this question.\n",
    "    The document should be approximately {desired_length} characters long and provide an in-depth, \n",
    "    informative answer to the question. Write as if this document is from an authoritative source\n",
    "    on the subject. Include specific details, facts, and explanations.\n",
    "    Do not mention that this is a hypothetical document - just write the content directly.\"\"\"\n",
    "\n",
    "    # Define the user prompt with the query\n",
    "    user_prompt = f\"Question: {query}\\n\\nGenerate a document that fully answers this question:\"\n",
    "    \n",
    "    # Make a request to the OpenAI API to generate the hypothetical document\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the query\n",
    "        ],\n",
    "        temperature=0.1  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated document content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, relevant_chunks):\n",
    "    \"\"\"\n",
    "    Generate a final response based on the query and relevant chunks.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        relevant_chunks (List[Dict]): Retrieved relevant chunks\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Concatenate the text from the chunks to create context\n",
    "    context = \"\\n\\n\".join([chunk[\"text\"] for chunk in relevant_chunks])\n",
    "    \n",
    "    # Generate response using OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_rag(query, pdf_path, k=5, should_generate_response=True):\n",
    "    \"\"\"\n",
    "    Perform RAG using Hypothetical Document Embedding.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        k (int): Number of chunks to retrieve\n",
    "        generate_response (bool): Whether to generate a final response\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including hypothetical document and retrieved chunks\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query with HyDE: {query} ===\\n\")\n",
    "    \n",
    "    # Step 1: Generate a hypothetical document that answers the query\n",
    "    print(\"Generating hypothetical document...\")\n",
    "    hypothetical_doc = generate_hypothetical_document(query)\n",
    "    print(f\"Generated hypothetical document of {len(hypothetical_doc)} characters\")\n",
    "    \n",
    "    # Step 2: Create embedding for the hypothetical document\n",
    "    print(\"Creating embedding for hypothetical document...\")\n",
    "    hypothetical_embedding = create_embeddings([hypothetical_doc])[0]\n",
    "    \n",
    "    # Step 3: Retrieve similar chunks based on the hypothetical document\n",
    "    print(f\"Retrieving {k} most similar chunks...\")\n",
    "    vector_store = process_document(pdf_path)  # Process the document to create a vector store\n",
    "    retrieved_chunks = vector_store.similarity_search(hypothetical_embedding, k=k)\n",
    "    \n",
    "    # Prepare the results dictionary\n",
    "    results = {\n",
    "        \"query\": query,\n",
    "        \"hypothetical_document\": hypothetical_doc,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    }\n",
    "    \n",
    "    # Step 4: Generate a response if requested\n",
    "    if should_generate_response:\n",
    "        print(\"Generating final response...\")\n",
    "        response = generate_response(query, retrieved_chunks)\n",
    "        results[\"response\"] = response\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing query with HyDE: How do transformers handle sequential data compared to RNNs? ===\n",
      "\n",
      "Generating hypothetical document...\n",
      "Generated hypothetical document of 1692 characters\n",
      "Creating embedding for hypothetical document...\n",
      "Retrieving 6 most similar chunks...\n",
      "Extracting text from AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Created 4 text chunks\n",
      "Created 4 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 3 text chunks\n",
      "Created 4 text chunks\n",
      "Created 3 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Vector store created with 48 chunks\n",
      "Generating final response...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'How do transformers handle sequential data compared to RNNs?',\n",
       " 'hypothetical_document': 'Transformers and Recurrent Neural Networks (RNNs) are both used for handling sequential data in machine learning and natural language processing tasks. However, they have different approaches to processing sequential data.\\n\\nRNNs process sequential data by iterating through the input sequence one element at a time, maintaining a hidden state that captures information about the sequence seen so far. This allows RNNs to capture dependencies and patterns in sequential data. However, RNNs suffer from issues such as vanishing gradients and difficulty in capturing long-range dependencies.\\n\\nOn the other hand, transformers handle sequential data using an attention mechanism that allows them to capture dependencies between all elements in the sequence simultaneously. This attention mechanism enables transformers to capture long-range dependencies more effectively than RNNs. Additionally, transformers can be parallelized more efficiently, leading to faster training times compared to RNNs.\\n\\nFurthermore, transformers have been shown to outperform RNNs in various natural language processing tasks, such as machine translation and language modeling. The self-attention mechanism in transformers allows them to capture relationships between words in a sentence regardless of their position, which is a significant advantage over RNNs.\\n\\nIn summary, while RNNs process sequential data by iterating through the input sequence one element at a time, transformers use an attention mechanism to capture dependencies between all elements in the sequence simultaneously, making them more effective at handling long-range dependencies and outperforming RNNs in many natural language processing tasks.',\n",
       " 'retrieved_chunks': [{'text': 'cessing images and videos. \\nThey use convolutional layers to automatically learn features from the input data. CNNs are \\nwidely used in object detection, facial recognition, and medical image analysis. \\nRecurrent Neural Networks (RNNs) \\nRNNs are designed to process sequential data, such as text and time series. They have feedback \\nconnections that allow information to persist over time, making them suitable for tasks like \\nlanguage translation, speech recognition, and sentiment analysis. \\nNatural Language Processing (NLP) \\nNatural Language Processing (NLP) is a branch of AI that focuses on enabling computers to \\nunderstand, interpret, and generate human language. NLP techniques are used in chatbots, \\nmachine translation, text summarization, and sentiment analysis. \\nComputer Vision \\nComputer vision is a field of AI that enables computers to \"see\" and interpret images and videos. \\nThis involves tasks such as object detection, image segmentation, and facial recognition. \\nComputer vision i',\n",
       "   'metadata': {'start_pos': 800,\n",
       "    'end_pos': 1800,\n",
       "    'source': 'AI_Information.pdf',\n",
       "    'page': 2},\n",
       "   'similarity': 0.5738683938980103},\n",
       "  {'text': 'Reinforcement Learning \\nReinforcement learning involves training an agent to make decisions in an environment to \\nmaximize a reward. The agent learns through trial and error, receiving feedback in the form of \\nrewards or penalties. This approach is used in game playing, robotics, and resource \\nmanagement. \\nDeep Learning \\nDeep learning is a subfield of machine learning that uses artificial neural networks with multiple \\nlayers (deep neural networks) to analyze data. These networks are inspired by the structure and \\nfunction of the human brain. Deep learning has achieved significant breakthroughs in areas such \\nas image recognition, natural language processing, and speech recognition. \\nConvolutional Neural Networks (CNNs) \\nCNNs are a type of deep neural network particularly effective for processing images and videos. \\nThey use convolutional layers to automatically learn features from the input data. CNNs are \\nwidely used in object detection, facial recognition, and medical image analysis',\n",
       "   'metadata': {'start_pos': 0,\n",
       "    'end_pos': 1000,\n",
       "    'source': 'AI_Information.pdf',\n",
       "    'page': 2},\n",
       "   'similarity': 0.4184767007827759},\n",
       "  {'text': 'Energy Management \\nAI optimizes energy management in smart cities by predicting demand, managing supply, and \\npromoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, \\nand support the integration of renewable energy sources. \\nPublic Safety and Security \\nAI enhances public safety and security in smart cities by monitoring public spaces, detecting \\nanomalies, and supporting emergency response. AI-powered systems improve crime prevention, \\nenhance situational awareness, and support rapid response to incidents. \\nEnvironmental Monitoring \\nAI-powered environmental monitoring systems track air and water quality, detect pollution, and \\nsupport environmental protection efforts. These systems provide real-time data, identify \\npollution sources, and inform environmental policies. \\nChapter 15: The Future of AI Research \\nAdvancements in Deep Learning \\nContinued advancements in deep learning are expected to drive further breakthroughs in AI. \\nResearch is focused o',\n",
       "   'metadata': {'start_pos': 0,\n",
       "    'end_pos': 1000,\n",
       "    'source': 'AI_Information.pdf',\n",
       "    'page': 11},\n",
       "   'similarity': 0.3213857412338257},\n",
       "  {'text': ' \\nTransportation \\nAI is revolutionizing transportation with the development of self-driving cars, traffic optimization \\nsystems, and logistics management. Autonomous vehicles use AI to perceive their surroundings, \\nmake driving decisions, and navigate safely. \\nRetail \\nThe retail industry uses AI for personalized recommendations, inventory management, customer \\nservice chatbots, and supply chain optimization. AI-powered systems can analyze customer data \\nto predict demand, personalize offers, and improve the shopping experience. \\nManufacturing \\nAI is used in manufacturing for predictive maintenance, quality control, process optimization, \\nand robotics. AI-powered systems can monitor equipment, detect anomalies, and automate \\ntasks, leading to increased efficiency and reduced costs. \\nEducation \\nAI is enhancing education through personalized learning platforms, automated grading systems, \\nand virtual tutors. AI-powered tools can adapt to individual student needs, provide feedback, and \\ncr',\n",
       "   'metadata': {'start_pos': 0,\n",
       "    'end_pos': 1000,\n",
       "    'source': 'AI_Information.pdf',\n",
       "    'page': 3},\n",
       "   'similarity': 0.2995419204235077},\n",
       "  {'text': 'AI is used in HR for talent acquisition, employee onboarding, performance management, and \\ntraining. AI-powered tools automate recruitment processes, personalize training programs, and \\nprovide insights into employee engagement and retention. \\nMarketing and Sales \\nAI enhances marketing and sales efforts by analyzing customer data, personalizing marketing \\ncampaigns, and predicting sales trends. AI-powered tools improve targeting, optimize ad \\nspending, and enhance customer segmentation. \\nFinancial Services \\nAI is used in financial services for fraud detection, risk management, algorithmic trading, and \\ncustomer service. AI-powered systems analyze large datasets to identify patterns, predict market \\nmovements, and automate financial processes. \\nChapter 8: AI and the Future of Work \\nAutomation and Job Displacement \\nThe increasing capabilities of AI raise concerns about job displacement, particularly in industries \\nwith repetitive or routine tasks. While AI may automate some jobs, it also',\n",
       "   'metadata': {'start_pos': 0,\n",
       "    'end_pos': 1000,\n",
       "    'source': 'AI_Information.pdf',\n",
       "    'page': 7},\n",
       "   'similarity': 0.2892220616340637},\n",
       "  {'text': ' personalized customer experiences, predicting customer \\nbehavior, and automating customer service interactions. AI-powered chatbots, recommendation \\nengines, and sentiment analysis tools improve customer engagement and satisfaction. \\nSupply Chain Management \\nAI optimizes supply chain operations by predicting demand, managing inventory, and \\nstreamlining logistics. AI-powered systems improve forecasting accuracy, reduce waste, and \\nenhance supply chain resilience. \\nHuman Resources (HR) \\n',\n",
       "   'metadata': {'start_pos': 1600,\n",
       "    'end_pos': 2092,\n",
       "    'source': 'AI_Information.pdf',\n",
       "    'page': 6},\n",
       "   'similarity': 0.28424111008644104}],\n",
       " 'response': 'Transformers handle sequential data differently compared to RNNs. While RNNs have feedback connections that allow information to persist over time, making them suitable for tasks like language translation, speech recognition, and sentiment analysis, transformers use a self-attention mechanism to process sequences of data in parallel, allowing them to capture dependencies between different elements of the sequence more effectively. This makes transformers more efficient for processing long-range dependencies in sequential data and has led to their widespread use in natural language processing tasks such as machine translation, text summarization, and language modeling.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"AI_Information.pdf\"  # Path to the PDF document\n",
    "query = \"How do transformers handle sequential data compared to RNNs?\"  # User's question\n",
    "\n",
    "hyde_rag(query=query, pdf_path=pdf_path, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
