{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Fazer o embedding usando esse\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it more specific and detailed for better retrieval.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for query rewriting\n",
    "        \n",
    "    Returns:\n",
    "        str: The rewritten query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be rewritten\n",
    "    user_prompt = f\"\"\"\n",
    "    Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the rewritten query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,  # Low temperature for deterministic output\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the rewritten query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generates a more general 'step-back' query to retrieve broader context.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original user query\n",
    "        model (str): The model to use for step-back query generation\n",
    "        \n",
    "    Returns:\n",
    "        str: The step-back query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be generalized\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a broader, more general version of the following query that could help retrieve useful background information.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Step-back query:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the step-back query using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Return the step-back query, stripping any leading/trailing whitespace\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Decomposes a complex query into simpler sub-queries.\n",
    "    \n",
    "    Args:\n",
    "        original_query (str): The original complex query\n",
    "        num_subqueries (int): Number of sub-queries to generate\n",
    "        model (str): The model to use for query decomposition\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of simpler sub-queries\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant's behavior\n",
    "    system_prompt = \"You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.\"\n",
    "    \n",
    "    # Define the user prompt with the original query to be decomposed\n",
    "    user_prompt = f\"\"\"\n",
    "    Break down the following complex query into {num_subqueries} simpler sub-queries. Each sub-query should focus on a different aspect of the original question.\n",
    "    \n",
    "    Original query: {original_query}\n",
    "    \n",
    "    Generate {num_subqueries} sub-queries, one per line, in this format:\n",
    "    1. [First sub-query]\n",
    "    2. [Second sub-query]\n",
    "    And so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the sub-queries using the specified model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,  # Slightly higher temperature for some variation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Process the response to extract sub-queries\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numbered queries using simple parsing\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # Remove the number and leading space\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "    \n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What are the impacts of AI on job automation and employment?\n",
      "\n",
      "1. Rewritten Query:\n",
      "What are the specific industries or job roles that are most affected by AI-driven automation, and what are the potential implications for employment rates and job displacement?\n",
      "\n",
      "2. Step-back Query:\n",
      "How does technology impact the workforce and employment trends?\n",
      "\n",
      "3. Sub-queries:\n",
      "   1. What are the specific job roles or industries most affected by AI-driven automation?\n",
      "   2. How does AI impact the creation of new job opportunities in various sectors?\n",
      "   3. What are the potential societal and economic implications of widespread job automation due to AI?\n",
      "   4. How do government policies and regulations influence the relationship between AI, job automation, and employment?\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "original_query = \"What are the impacts of AI on job automation and employment?\"\n",
    "\n",
    "# Apply query transformations\n",
    "print(\"Original Query:\", original_query)\n",
    "\n",
    "# Query Rewriting\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\n1. Rewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# Step-back Prompting\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\n2. Step-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# Sub-query Decomposition\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\n3. Sub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(text), n-overlap):\n",
    "        chunks.append(text[i:i+n])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text_chunk, num_questions=5, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generates relevant questions that can be answered from the given text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunk (str): The text chunk to generate questions from.\n",
    "    num_questions (int): Number of questions to generate.\n",
    "    model (str): The model to use for question generation.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of generated questions.\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n",
    "    \n",
    "    # Define the user prompt with the text chunk and the number of questions to generate\n",
    "    user_prompt = f\"\"\"\n",
    "    Based on the following text, generate {num_questions} different questions that can be answered using only this text:\n",
    "\n",
    "    {text_chunk}\n",
    "    \n",
    "    Format your response as a numbered list of questions only, with no additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate questions using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract and clean questions from the response\n",
    "    questions_text = response.choices[0].message.content.strip()\n",
    "    questions = []\n",
    "    \n",
    "    # Extract questions using regex pattern matching\n",
    "    for line in questions_text.split('\\n'):\n",
    "        # Remove numbering and clean up whitespace\n",
    "        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "        if cleaned_line and cleaned_line.endswith('?'):\n",
    "            questions.append(cleaned_line)\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    response = embedder.encode(text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(embedding)\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    # Create embeddings for all chunks at once for efficiency\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add chunks to vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, k):\n",
    "    \"\"\"\n",
    "    Search using a transformed query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query\n",
    "        vector_store (SimpleVectorStore): Vector store to search\n",
    "        transformation_type (str): Type of transformation ('rewrite', 'step_back', or 'decompose')\n",
    "        top_k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Search results\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if transformation_type == \"rewrite\":\n",
    "        # Query rewriting\n",
    "        transformed_query = rewrite_query(query)\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with rewritten query\n",
    "        results = vector_store.similarity_search(query_embedding, k=k)\n",
    "        \n",
    "    elif transformation_type == \"step_back\":\n",
    "        # Step-back prompting\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        \n",
    "        # Create embedding for transformed query\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        \n",
    "        # Search with step-back query\n",
    "        results = vector_store.similarity_search(query_embedding, k=k)\n",
    "        \n",
    "    elif transformation_type == \"decompose\":\n",
    "        # Sub-query decomposition\n",
    "        sub_queries = decompose_query(query)\n",
    "        \n",
    "        # Create embeddings for all sub-queries\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        \n",
    "        # Search with each sub-query and combine results\n",
    "        all_results = []\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=k)  # Get fewer results per sub-query\n",
    "            all_results.extend(sub_results)\n",
    "        \n",
    "        # Remove duplicates (keep highest similarity score)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "        \n",
    "        # Sort by similarity and take top_k\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:k]\n",
    "        \n",
    "    else:\n",
    "        # Regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(search_results):\n",
    "    \"\"\"\n",
    "    Prepares a unified context from search results for response generation.\n",
    "\n",
    "    Args:\n",
    "    search_results (List[Dict]): Results from semantic search.\n",
    "\n",
    "    Returns:\n",
    "    str: Combined context string.\n",
    "    \"\"\"\n",
    "    # Extract unique chunks referenced in the results\n",
    "    chunk_indices = set()\n",
    "    context_chunks = []\n",
    "    \n",
    "    # First add direct chunk matches\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "            chunk_indices.add(result[\"metadata\"][\"index\"])\n",
    "            context_chunks.append(f\"Chunk {result['metadata']['index']}:\\n{result['text']}\")\n",
    "    \n",
    "    # Then add chunks referenced by questions\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"question\":\n",
    "            chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
    "            if chunk_idx not in chunk_indices:\n",
    "                chunk_indices.add(chunk_idx)\n",
    "                context_chunks.append(f\"Chunk {chunk_idx} (referenced by question '{result['text']}'):\\n{result['metadata']['original_chunk']}\")\n",
    "    \n",
    "    # Combine all context chunks\n",
    "    full_context = \"\\n\\n\".join(context_chunks)\n",
    "    return full_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and context.\n",
    "\n",
    "    Args:\n",
    "    query (str): User's question.\n",
    "    context (str): Context information retrieved from the vector store.\n",
    "    model (str): Model to use for response generation.\n",
    "\n",
    "    Returns:\n",
    "    str: Generated response.\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    Run complete RAG pipeline with optional query transformation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        transformation_type (str): Type of transformation (None, 'rewrite', 'step_back', or 'decompose')\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, transformed query, context, and response\n",
    "    \"\"\"\n",
    "    # Process the document to create a vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Apply query transformation and search\n",
    "    if transformation_type:\n",
    "        # Perform search with transformed query\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # Perform regular search without transformation\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "    \n",
    "    # Combine context from search results\n",
    "    context = \"\\n\\n\".join([f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)])\n",
    "    \n",
    "    # Generate response based on the query and combined context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the results including original query, transformation type, context, and response\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the SentenceTransformer model and set the similarity function\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model.similarity_fn_name = SimilarityFunction.DOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = extract_text_from_pdf(\"AI_Information.pdf\")\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = SimpleVectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_validation_data(k, transformation_type=\"rewrite\"):\n",
    "    system_prompt = \"\"\"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly and exactly from the provided context, respond with: 'I do not have enough information to answer that.'\n",
    "    First think about the keywords from the question and then use them to elaborate the answer.\n",
    "    The response needs to be just the answer sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load the validation data from the JSON file\n",
    "    with open('val.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # List to store the results for each sample\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each example in the validation data\n",
    "    for idx, item in enumerate(data):\n",
    "        query = item['question']\n",
    "        ideal_answer = item['ideal_answer']\n",
    "        \n",
    "        # Retrieve the top k most relevant context chunks\n",
    "        top_chunks = transformed_search(query, vector_store, transformation_type, k=k)\n",
    "        \n",
    "        # Create the user prompt by combining all context chunks and the query\n",
    "        context_prompt = \"\\n\".join([\n",
    "            f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\"\n",
    "            for i, chunk in enumerate(top_chunks)\n",
    "        ])\n",
    "        user_prompt = f\"{context_prompt}\\nQuestion: {query}\"\n",
    "        \n",
    "        # Generate the AI response using the system prompt and the user prompt\n",
    "        ai_response = generate_response(system_prompt, user_prompt)\n",
    "        \n",
    "        # Evaluate similarity using SentenceTransformer\n",
    "        # Encode the AI response and ideal answer\n",
    "        embedding_response = model.encode([ai_response])\n",
    "        embedding_ideal = model.encode([ideal_answer])\n",
    "        # Compute similarity score (result is a 1x1 matrix; extract the single value)\n",
    "        similarity_matrix = model.similarity(embedding_response, embedding_ideal)\n",
    "        score = similarity_matrix[0][0].numpy()\n",
    "        \n",
    "        # Prepare the result dictionary with dynamic context columns\n",
    "        result = {\n",
    "            \"Query\": query,\n",
    "            \"Ideal Answer\": ideal_answer,\n",
    "            \"AI Response\": ai_response,\n",
    "            \"Score\": score\n",
    "        }\n",
    "        # Add each context as its own column\n",
    "        for i, chunk in enumerate(top_chunks):\n",
    "            result[f\"Context {i + 1}\"] = chunk\n",
    "        \n",
    "        # Append the result to the list\n",
    "        results.append(result)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41870908737182616\n",
      "0.41870908737182616\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.41870908737182616\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 9):\n",
    "    print(asyncio.run(process_validation_data(k=k))['Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 9):\n",
    "    print(asyncio.run(process_validation_data(k=k, transformation_type='step_back'))['Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.41870908737182616\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n",
      "0.49298410415649413\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 9):\n",
    "    print(asyncio.run(process_validation_data(k=k, transformation_type='decompose'))['Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
