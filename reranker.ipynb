{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Fazer o embedding usando esse\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(text), n-overlap):\n",
    "        chunks.append(text[i:i+n])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text_chunk, num_questions=5, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generates relevant questions that can be answered from the given text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunk (str): The text chunk to generate questions from.\n",
    "    num_questions (int): Number of questions to generate.\n",
    "    model (str): The model to use for question generation.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of generated questions.\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n",
    "    \n",
    "    # Define the user prompt with the text chunk and the number of questions to generate\n",
    "    user_prompt = f\"\"\"\n",
    "    Based on the following text, generate {num_questions} different questions that can be answered using only this text:\n",
    "\n",
    "    {text_chunk}\n",
    "    \n",
    "    Format your response as a numbered list of questions only, with no additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate questions using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract and clean questions from the response\n",
    "    questions_text = response.choices[0].message.content.strip()\n",
    "    questions = []\n",
    "    \n",
    "    # Extract questions using regex pattern matching\n",
    "    for line in questions_text.split('\\n'):\n",
    "        # Remove numbering and clean up whitespace\n",
    "        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "        if cleaned_line and cleaned_line.endswith('?'):\n",
    "            questions.append(cleaned_line)\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    response = embedder.encode(text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(embedding)\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for RAG.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    SimpleVectorStore: A vector store containing document chunks and their embeddings.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for the text chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize a simple vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its embedding to the vector store\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_llm(query, results, top_n=3, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Reranks search results using LLM relevance scoring.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Initial search results\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        model (str): Model to use for scoring\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Reranked results\n",
    "    \"\"\"\n",
    "    print(f\"Reranking {len(results)} documents...\")  # Print the number of documents to be reranked\n",
    "    \n",
    "    scored_results = []  # Initialize an empty list to store scored results\n",
    "    \n",
    "    # Define the system prompt for the LLM\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance for search queries.\n",
    "Your task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.\n",
    "\n",
    "Guidelines:\n",
    "- Score 0-2: Document is completely irrelevant\n",
    "- Score 3-5: Document has some relevant information but doesn't directly answer the query\n",
    "- Score 6-8: Document is relevant and partially answers the query\n",
    "- Score 9-10: Document is highly relevant and directly answers the query\n",
    "\n",
    "You MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.\"\"\"\n",
    "    \n",
    "    # Iterate through each result\n",
    "    for i, result in enumerate(results):\n",
    "        # Show progress every 5 documents\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Scoring document {i+1}/{len(results)}...\")\n",
    "        \n",
    "        # Define the user prompt for the LLM\n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "Document:\n",
    "{result['text']}\n",
    "\n",
    "Rate this document's relevance to the query on a scale from 0 to 10:\"\"\"\n",
    "        \n",
    "        # Get the LLM response\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the score from the LLM response\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Use regex to extract the numerical score\n",
    "        score_match = re.search(r'\\b(10|[0-9])\\b', score_text)\n",
    "        if score_match:\n",
    "            score = float(score_match.group(1))\n",
    "        else:\n",
    "            # If score extraction fails, use similarity score as fallback\n",
    "            print(f\"Warning: Could not extract score from response: '{score_text}', using similarity score instead\")\n",
    "            score = result[\"similarity\"] * 10\n",
    "        \n",
    "        # Append the scored result to the list\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": score\n",
    "        })\n",
    "    \n",
    "    # Sort results by relevance score in descending order\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top_n results\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_keywords(query, results, top_n=3):\n",
    "    \"\"\"\n",
    "    A simple alternative reranking method based on keyword matching and position.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Initial search results\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Reranked results\n",
    "    \"\"\"\n",
    "    # Extract important keywords from the query\n",
    "    keywords = [word.lower() for word in query.split() if len(word) > 3]\n",
    "    \n",
    "    scored_results = []  # Initialize a list to store scored results\n",
    "    \n",
    "    for result in results:\n",
    "        document_text = result[\"text\"].lower()  # Convert document text to lowercase\n",
    "        \n",
    "        # Base score starts with vector similarity\n",
    "        base_score = result[\"similarity\"] * 0.5\n",
    "        \n",
    "        # Initialize keyword score\n",
    "        keyword_score = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword in document_text:\n",
    "                # Add points for each keyword found\n",
    "                keyword_score += 0.1\n",
    "                \n",
    "                # Add more points if keyword appears near the beginning\n",
    "                first_position = document_text.find(keyword)\n",
    "                if first_position < len(document_text) / 4:  # In the first quarter of the text\n",
    "                    keyword_score += 0.1\n",
    "                \n",
    "                # Add points for keyword frequency\n",
    "                frequency = document_text.count(keyword)\n",
    "                keyword_score += min(0.05 * frequency, 0.2)  # Cap at 0.2\n",
    "        \n",
    "        # Calculate the final score by combining base score and keyword score\n",
    "        final_score = base_score + keyword_score\n",
    "        \n",
    "        # Append the scored result to the list\n",
    "        scored_results.append({\n",
    "            \"text\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"similarity\": result[\"similarity\"],\n",
    "            \"relevance_score\": final_score\n",
    "        })\n",
    "    \n",
    "    # Sort results by final relevance score in descending order\n",
    "    reranked_results = sorted(scored_results, key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top_n results\n",
    "    return reranked_results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"AI_Information.pdf\"\n",
    "\n",
    "# Process the document (extract text, create chunks, generate questions, build vector store)\n",
    "vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search using the query and vector store.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query.\n",
    "    vector_store (SimpleVectorStore): The vector store to search in.\n",
    "    k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: Top k most relevant items.\n",
    "    \"\"\"\n",
    "    # Create embedding for the query\n",
    "    query_embedding_response = create_embeddings(query)\n",
    "    query_embedding = query_embedding_response\n",
    "    \n",
    "    # Search the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and context.\n",
    "\n",
    "    Args:\n",
    "    query (str): User's question.\n",
    "    context (str): Context information retrieved from the vector store.\n",
    "    model (str): Model to use for response generation.\n",
    "\n",
    "    Returns:\n",
    "    str: Generated response.\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_reranking(query, vector_store, reranking_method=\"llm\", top_n=3, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline incorporating reranking.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        reranking_method (str): Method for reranking ('llm' or 'keywords')\n",
    "        top_n (int): Number of results to return after reranking\n",
    "        model (str): Model for response generation\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, context, and response\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Initial retrieval (get more than we need for reranking)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=10)\n",
    "    \n",
    "    # Apply reranking\n",
    "    if reranking_method == \"llm\":\n",
    "        reranked_results = rerank_with_llm(query, initial_results, top_n=top_n)\n",
    "    elif reranking_method == \"keywords\":\n",
    "        reranked_results = rerank_with_keywords(query, initial_results, top_n=top_n)\n",
    "    else:\n",
    "        # No reranking, just use top results from initial retrieval\n",
    "        reranked_results = initial_results[:top_n]\n",
    "    \n",
    "    # Combine context from reranked results\n",
    "    context = \"\\n\\n===\\n\\n\".join([result[\"text\"] for result in reranked_results])\n",
    "    \n",
    "    # Generate response based on context\n",
    "    response = generate_response(query, context, model)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"reranking_method\": reranking_method,\n",
    "        \"initial_results\": initial_results[:top_n],\n",
    "        \"reranked_results\": reranked_results,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the SentenceTransformer model and set the similarity function\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model.similarity_fn_name = SimilarityFunction.DOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_validation_data(k, reranking_method):\n",
    "    system_prompt = \"\"\"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly and exactly from the provided context, respond with: 'I do not have enough information to answer that.'\n",
    "    First think about the keywords from the question and then use them to elaborate the answer.\n",
    "    The response needs to be just the answer sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load the validation data from the JSON file\n",
    "    with open('val.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # List to store the results for each sample\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each example in the validation data\n",
    "    for idx, item in enumerate(data):\n",
    "        query = item['question']\n",
    "        ideal_answer = item['ideal_answer']\n",
    "        \n",
    "        # Retrieve the top k most relevant context chunks\n",
    "                \n",
    "        # Generate the AI response using the system prompt and the user prompt\n",
    "        result = rag_with_reranking(query, vector_store, reranking_method=reranking_method)\n",
    "        ai_response = result['response']\n",
    "        # Evaluate similarity using SentenceTransformer\n",
    "        # Encode the AI response and ideal answer\n",
    "        embedding_response = model.encode([ai_response])\n",
    "        embedding_ideal = model.encode([ideal_answer])\n",
    "        # Compute similarity score (result is a 1x1 matrix; extract the single value)\n",
    "        similarity_matrix = model.similarity(embedding_response, embedding_ideal)\n",
    "        score = similarity_matrix[0][0].numpy()\n",
    "        \n",
    "        # Prepare the result dictionary with dynamic context columns\n",
    "        result = {\n",
    "            \"Query\": query,\n",
    "            \"Ideal Answer\": ideal_answer,\n",
    "            \"AI Response\": ai_response,\n",
    "            \"Score\": score\n",
    "        }\n",
    "        \n",
    "        # Append the result to the list\n",
    "        results.append(result)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6648365974426269\n",
      "0.66173095703125\n",
      "0.6660214424133301\n",
      "0.6639578819274903\n",
      "0.6637609481811524\n",
      "0.6628089427947998\n",
      "0.6661837577819825\n",
      "0.6626096725463867\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 9):\n",
    "    print(asyncio.run(process_validation_data(k=k, reranking_method='keywords'))['Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
