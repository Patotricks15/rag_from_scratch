{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Fazer o embedding usando esse\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file with page separation.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of pages with text content and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    pages = []  # Initialize an empty list to store the pages with text content\n",
    "    \n",
    "    # Iterate over each page in the PDF\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]  # Get the current page\n",
    "        text = page.get_text()  # Extract text from the current page\n",
    "        \n",
    "        # Skip pages with very little text (less than 50 characters)\n",
    "        if len(text.strip()) > 50:\n",
    "            # Append the page text and metadata to the list\n",
    "            pages.append({\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": pdf_path,  # Source file path\n",
    "                    \"page\": page_num + 1  # Page number (1-based index)\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"Extracted {len(pages)} pages with content\")  # Print the number of pages extracted\n",
    "    return pages  # Return the list of pages with text content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, metadata, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks while preserving metadata.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        metadata (Dict): Metadata to preserve\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of text chunks with metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text = text[i:i + chunk_size]  # Extract the chunk of text\n",
    "        \n",
    "        # Skip very small chunks (less than 50 characters)\n",
    "        if chunk_text and len(chunk_text.strip()) > 50:\n",
    "            # Create a copy of metadata and add chunk-specific info\n",
    "            chunk_metadata = metadata.copy()\n",
    "            chunk_metadata.update({\n",
    "                \"chunk_index\": len(chunks),  # Index of the chunk\n",
    "                \"start_char\": i,  # Start character index of the chunk\n",
    "                \"end_char\": i + len(chunk_text),  # End character index of the chunk\n",
    "                \"is_summary\": False  # Flag indicating this is not a summary\n",
    "            })\n",
    "            \n",
    "            # Append the chunk with its metadata to the list\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": chunk_metadata\n",
    "            })\n",
    "    \n",
    "    return chunks  # Return the list of chunks with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store vector embeddings\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text content\n",
    "            embedding (List[float]): Vector embedding\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Append the embedding as a numpy array\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata or an empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            filter_func (callable, optional): Function to filter results\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return an empty list if there are no vectors\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Skip if doesn't pass the filter\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text content\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": float(score)  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given text.\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s)\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vector(s)\n",
    "    \"\"\"\n",
    "    # Create embeddings for the current batch\n",
    "    response = embedder.encode(text)\n",
    "    return response  # Return the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_page_summary(page_text):\n",
    "    \"\"\"\n",
    "    Generate a concise summary of a page.\n",
    "    \n",
    "    Args:\n",
    "        page_text (str): Text content of the page\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated summary\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the summarization model\n",
    "    system_prompt = \"\"\"You are an expert summarization system.\n",
    "    Create a detailed summary of the provided text. \n",
    "    Focus on capturing the main topics, key information, and important facts.\n",
    "    Your summary should be comprehensive enough to understand what the page contains\n",
    "    but more concise than the original.\"\"\"\n",
    "\n",
    "    # Truncate input text if it exceeds the maximum token limit\n",
    "    max_tokens = 6000\n",
    "    truncated_text = page_text[:max_tokens] if len(page_text) > max_tokens else page_text\n",
    "\n",
    "    # Make a request to the OpenAI API to generate the summary\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Please summarize this text:\\n\\n{truncated_text}\"}  # User message with the text to summarize\n",
    "        ],\n",
    "        temperature=0.3  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated summary content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_hierarchically(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document into hierarchical indices.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each detailed chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[SimpleVectorStore, SimpleVectorStore]: Summary and detailed vector stores\n",
    "    \"\"\"\n",
    "    # Extract pages from PDF\n",
    "    pages = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create summaries for each page\n",
    "    print(\"Generating page summaries...\")\n",
    "    summaries = []\n",
    "    for i, page in enumerate(pages):\n",
    "        print(f\"Summarizing page {i+1}/{len(pages)}...\")\n",
    "        summary_text = generate_page_summary(page[\"text\"])\n",
    "        \n",
    "        # Create summary metadata\n",
    "        summary_metadata = page[\"metadata\"].copy()\n",
    "        summary_metadata.update({\"is_summary\": True})\n",
    "        \n",
    "        # Append the summary text and metadata to the summaries list\n",
    "        summaries.append({\n",
    "            \"text\": summary_text,\n",
    "            \"metadata\": summary_metadata\n",
    "        })\n",
    "    \n",
    "    # Create detailed chunks for each page\n",
    "    detailed_chunks = []\n",
    "    for page in pages:\n",
    "        # Chunk the text of the page\n",
    "        page_chunks = chunk_text(\n",
    "            page[\"text\"], \n",
    "            page[\"metadata\"], \n",
    "            chunk_size, \n",
    "            chunk_overlap\n",
    "        )\n",
    "        # Extend the detailed_chunks list with the chunks from the current page\n",
    "        detailed_chunks.extend(page_chunks)\n",
    "    \n",
    "    print(f\"Created {len(detailed_chunks)} detailed chunks\")\n",
    "    \n",
    "    # Create embeddings for summaries\n",
    "    print(\"Creating embeddings for summaries...\")\n",
    "    summary_texts = [summary[\"text\"] for summary in summaries]\n",
    "    summary_embeddings = create_embeddings(summary_texts)\n",
    "    \n",
    "    # Create embeddings for detailed chunks\n",
    "    print(\"Creating embeddings for detailed chunks...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in detailed_chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Create vector stores\n",
    "    summary_store = SimpleVectorStore()\n",
    "    detailed_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add summaries to summary store\n",
    "    for i, summary in enumerate(summaries):\n",
    "        summary_store.add_item(\n",
    "            text=summary[\"text\"],\n",
    "            embedding=summary_embeddings[i],\n",
    "            metadata=summary[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    # Add chunks to detailed store\n",
    "    for i, chunk in enumerate(detailed_chunks):\n",
    "        detailed_store.add_item(\n",
    "            text=chunk[\"text\"],\n",
    "            embedding=chunk_embeddings[i],\n",
    "            metadata=chunk[\"metadata\"]\n",
    "        )\n",
    "    \n",
    "    print(f\"Created vector stores with {len(summaries)} summaries and {len(detailed_chunks)} chunks\")\n",
    "    return summary_store, detailed_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_hierarchically(query, summary_store, detailed_store, k_summaries=3, k_chunks=5):\n",
    "    \"\"\"\n",
    "    Retrieve information using hierarchical indices.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        summary_store (SimpleVectorStore): Store of document summaries\n",
    "        detailed_store (SimpleVectorStore): Store of detailed chunks\n",
    "        k_summaries (int): Number of summaries to retrieve\n",
    "        k_chunks (int): Number of chunks to retrieve per summary\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved chunks with relevance scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing hierarchical retrieval for query: {query}\")\n",
    "    \n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # First, retrieve relevant summaries\n",
    "    summary_results = summary_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_summaries\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(summary_results)} relevant summaries\")\n",
    "    \n",
    "    # Collect pages from relevant summaries\n",
    "    relevant_pages = [result[\"metadata\"][\"page\"] for result in summary_results]\n",
    "    \n",
    "    # Create a filter function to only keep chunks from relevant pages\n",
    "    def page_filter(metadata):\n",
    "        return metadata[\"page\"] in relevant_pages\n",
    "    \n",
    "    # Then, retrieve detailed chunks from only those relevant pages\n",
    "    detailed_results = detailed_store.similarity_search(\n",
    "        query_embedding, \n",
    "        k=k_chunks * len(relevant_pages),\n",
    "        filter_func=page_filter\n",
    "    )\n",
    "    \n",
    "    print(f\"Retrieved {len(detailed_results)} detailed chunks from relevant pages\")\n",
    "    \n",
    "    # For each result, add which summary/page it came from\n",
    "    for result in detailed_results:\n",
    "        page = result[\"metadata\"][\"page\"]\n",
    "        matching_summaries = [s for s in summary_results if s[\"metadata\"][\"page\"] == page]\n",
    "        if matching_summaries:\n",
    "            result[\"summary\"] = matching_summaries[0][\"text\"]\n",
    "    \n",
    "    return detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and retrieved chunks.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        retrieved_chunks (List[Dict]): Retrieved chunks from hierarchical search\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Extract text from chunks and prepare context parts\n",
    "    context_parts = []\n",
    "    \n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        page_num = chunk[\"metadata\"][\"page\"]  # Get the page number from metadata\n",
    "        context_parts.append(f\"[Page {page_num}]: {chunk['text']}\")  # Format the chunk text with page number\n",
    "    \n",
    "    # Combine all context parts into a single context string\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Define the system message to guide the AI assistant\n",
    "    system_message = \"\"\"You are a helpful AI assistant answering questions based on the provided context.\n",
    "Use the information from the context to answer the user's question accurately.\n",
    "If the context doesn't contain relevant information, acknowledge that.\n",
    "Include page numbers when referencing specific information.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n\\n{context}\\n\\nQuestion: {query}\"}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.2  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_rag(query, pdf_path, chunk_size=1000, chunk_overlap=200, \n",
    "                    k_summaries=3, k_chunks=5, regenerate=False):\n",
    "    \"\"\"\n",
    "    Complete hierarchical RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each detailed chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k_summaries (int): Number of summaries to retrieve\n",
    "        k_chunks (int): Number of chunks to retrieve per summary\n",
    "        regenerate (bool): Whether to regenerate vector stores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including response and retrieved chunks\n",
    "    \"\"\"\n",
    "    # Create store filenames for caching\n",
    "    summary_store_file = f\"{os.path.basename(pdf_path)}_summary_store.pkl\"\n",
    "    detailed_store_file = f\"{os.path.basename(pdf_path)}_detailed_store.pkl\"\n",
    "    \n",
    "    # Process document and create stores if needed\n",
    "    if regenerate or not os.path.exists(summary_store_file) or not os.path.exists(detailed_store_file):\n",
    "        print(\"Processing document and creating vector stores...\")\n",
    "        # Process the document to create hierarchical indices and vector stores\n",
    "        summary_store, detailed_store = process_document_hierarchically(\n",
    "            pdf_path, chunk_size, chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Save the summary store to a file for future use\n",
    "        with open(summary_store_file, 'wb') as f:\n",
    "            pickle.dump(summary_store, f)\n",
    "        \n",
    "        # Save the detailed store to a file for future use\n",
    "        with open(detailed_store_file, 'wb') as f:\n",
    "            pickle.dump(detailed_store, f)\n",
    "    else:\n",
    "        # Load existing summary store from file\n",
    "        print(\"Loading existing vector stores...\")\n",
    "        with open(summary_store_file, 'rb') as f:\n",
    "            summary_store = pickle.load(f)\n",
    "        \n",
    "        # Load existing detailed store from file\n",
    "        with open(detailed_store_file, 'rb') as f:\n",
    "            detailed_store = pickle.load(f)\n",
    "    \n",
    "    # Retrieve relevant chunks hierarchically using the query\n",
    "    retrieved_chunks = retrieve_hierarchically(\n",
    "        query, summary_store, detailed_store, k_summaries, k_chunks\n",
    "    )\n",
    "    \n",
    "    # Generate a response based on the retrieved chunks\n",
    "    response = generate_response(query, retrieved_chunks)\n",
    "    \n",
    "    # Return results including the query, response, retrieved chunks, and counts of summaries and detailed chunks\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_chunks\": retrieved_chunks,\n",
    "        \"summary_count\": len(summary_store.texts),\n",
    "        \"detailed_count\": len(detailed_store.texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document and creating vector stores...\n",
      "Extracting text from AI_Information.pdf...\n",
      "Extracted 15 pages with content\n",
      "Generating page summaries...\n",
      "Summarizing page 1/15...\n",
      "Summarizing page 2/15...\n",
      "Summarizing page 3/15...\n",
      "Summarizing page 4/15...\n",
      "Summarizing page 5/15...\n",
      "Summarizing page 6/15...\n",
      "Summarizing page 7/15...\n",
      "Summarizing page 8/15...\n",
      "Summarizing page 9/15...\n",
      "Summarizing page 10/15...\n",
      "Summarizing page 11/15...\n",
      "Summarizing page 12/15...\n",
      "Summarizing page 13/15...\n",
      "Summarizing page 14/15...\n",
      "Summarizing page 15/15...\n",
      "Created 47 detailed chunks\n",
      "Creating embeddings for summaries...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m pdf_path = \u001b[33m\"\u001b[39m\u001b[33mAI_Information.pdf\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Path to the PDF document\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mHow do transformers handle sequential data compared to RNNs?\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# User's question\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mhierarchical_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mhierarchical_rag\u001b[39m\u001b[34m(query, pdf_path, chunk_size, chunk_overlap, k_summaries, k_chunks, regenerate)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing document and creating vector stores...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Process the document to create hierarchical indices and vector stores\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m summary_store, detailed_store = \u001b[43mprocess_document_hierarchically\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Save the summary store to a file for future use\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(summary_store_file, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mprocess_document_hierarchically\u001b[39m\u001b[34m(pdf_path, chunk_size, chunk_overlap)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating embeddings for summaries...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m summary_texts = [summary[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m summaries]\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m summary_embeddings = \u001b[43mcreate_embeddings\u001b[49m(summary_texts)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Create embeddings for detailed chunks\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating embeddings for detailed chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "pdf_path = \"AI_Information.pdf\"  # Path to the PDF document\n",
    "query = \"How do transformers handle sequential data compared to RNNs?\"  # User's question\n",
    "\n",
    "hierarchical_rag(query=query, pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
