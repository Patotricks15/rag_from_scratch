{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Fazer o embedding usando esse\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(text), chunk_size-overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    response = embedder.encode(text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "        filter_func (callable, optional): Function to filter results.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Apply filter if provided\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for use with adaptive retrieval.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for the text chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize the vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its embedding to the vector store with metadata\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    \n",
    "    # Return the chunks and the vector store\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(query, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Classify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        model (str): LLM model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Query category\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's classification\n",
    "    system_prompt = \"\"\"You are an expert at classifying questions. \n",
    "        Classify the given query into exactly one of these categories:\n",
    "        - Factual: Queries seeking specific, verifiable information.\n",
    "        - Analytical: Queries requiring comprehensive analysis or explanation.\n",
    "        - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
    "        - Contextual: Queries that depend on user-specific context.\n",
    "\n",
    "        Return ONLY the category name, without any explanation or additional text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the user prompt with the query to be classified\n",
    "    user_prompt = f\"Classify this query: {query}\"\n",
    "    \n",
    "    # Generate the classification response from the AI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract and strip the category from the response\n",
    "    category = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Define the list of valid categories\n",
    "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "    \n",
    "    # Ensure the returned category is valid\n",
    "    for valid in valid_categories:\n",
    "        if valid in category:\n",
    "            return valid\n",
    "    \n",
    "    # Default to \"Factual\" if classification fails\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for factual queries focusing on precision.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Use LLM to enhance the query for better precision\n",
    "    system_prompt = \"\"\"You are an expert at enhancing search queries.\n",
    "        Your task is to reformulate the given factual query to make it more precise and \n",
    "        specific for information retrieval. Focus on key entities and their relationships.\n",
    "\n",
    "        Provide ONLY the enhanced query without any explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Enhance this factual query: {query}\"\n",
    "    \n",
    "    # Generate the enhanced query using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract and print the enhanced query\n",
    "    enhanced_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    \n",
    "    # Create embeddings for the enhanced query\n",
    "    query_embedding = create_embeddings(enhanced_query)\n",
    "    \n",
    "    # Perform initial similarity search to retrieve documents\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Initialize a list to store ranked results\n",
    "    ranked_results = []\n",
    "    \n",
    "    # Score and rank documents by relevance using LLM\n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    # Sort the results by relevance score in descending order\n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for analytical queries focusing on comprehensive coverage.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Define the system prompt to guide the AI in generating sub-questions\n",
    "    system_prompt = \"\"\"You are an expert at breaking down complex questions.\n",
    "    Generate sub-questions that explore different aspects of the main analytical query.\n",
    "    These sub-questions should cover the breadth of the topic and help retrieve \n",
    "    comprehensive information.\n",
    "\n",
    "    Return a list of exactly 3 sub-questions, one per line.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the user prompt with the main query\n",
    "    user_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n",
    "    \n",
    "    # Generate the sub-questions using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Extract and clean the sub-questions\n",
    "    sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
    "    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "    print(f\"Generated sub-queries: {sub_queries}\")\n",
    "    \n",
    "    # Retrieve documents for each sub-query\n",
    "    all_results = []\n",
    "    for sub_query in sub_queries:\n",
    "        # Create embeddings for the sub-query\n",
    "        sub_query_embedding = create_embeddings(sub_query)\n",
    "        # Perform similarity search for the sub-query\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Ensure diversity by selecting from different sub-query results\n",
    "    # Remove duplicates (same text content)\n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result[\"text\"])\n",
    "            diverse_results.append(result)\n",
    "    \n",
    "    # If we need more results to reach k, add more from initial results\n",
    "    if len(diverse_results) < k:\n",
    "        # Direct retrieval for the main query\n",
    "        main_query_embedding = create_embeddings(query)\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result in main_results:\n",
    "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result[\"text\"])\n",
    "                diverse_results.append(result)\n",
    "    \n",
    "    # Return the top k diverse results\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for opinion queries focusing on diverse perspectives.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Define the system prompt to guide the AI in identifying different perspectives\n",
    "    system_prompt = \"\"\"You are an expert at identifying different perspectives on a topic.\n",
    "        For the given query about opinions or viewpoints, identify different perspectives \n",
    "        that people might have on this topic.\n",
    "\n",
    "        Return a list of exactly 3 different viewpoint angles, one per line.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the user prompt with the main query\n",
    "    user_prompt = f\"Identify different perspectives on: {query}\"\n",
    "    \n",
    "    # Generate the different perspectives using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Extract and clean the viewpoints\n",
    "    viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
    "    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
    "    print(f\"Identified viewpoints: {viewpoints}\")\n",
    "    \n",
    "    # Retrieve documents representing each viewpoint\n",
    "    all_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Combine the main query with the viewpoint\n",
    "        combined_query = f\"{query} {viewpoint}\"\n",
    "        # Create embeddings for the combined query\n",
    "        viewpoint_embedding = create_embeddings(combined_query)\n",
    "        # Perform similarity search for the combined query\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "        \n",
    "        # Mark results with the viewpoint they represent\n",
    "        for result in results:\n",
    "            result[\"viewpoint\"] = viewpoint\n",
    "        \n",
    "        # Add the results to the list of all results\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Select a diverse range of opinions\n",
    "    # Ensure we get at least one document from each viewpoint if possible\n",
    "    selected_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Filter documents by viewpoint\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "    \n",
    "    # Fill remaining slots with highest similarity docs\n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        # Sort remaining docs by similarity\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "    \n",
    "    # Return the top k results\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_relevance(query, document, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Score document relevance to a query using LLM.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        document (str): Document text\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        float: Relevance score from 0-10\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n",
    "        Rate the relevance of a document to a query on a scale from 0 to 10, where:\n",
    "        0 = Completely irrelevant\n",
    "        10 = Perfectly addresses the query\n",
    "\n",
    "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate document if it's too long\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # User prompt containing the query and document preview\n",
    "    user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document: {doc_preview}\n",
    "\n",
    "        Relevance score (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the score from the model's response\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numeric score using regex\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
    "    else:\n",
    "        # Default score if extraction fails\n",
    "        return 5.0\n",
    "    \n",
    "\n",
    "def score_document_context_relevance(query, context, document, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Score document relevance considering both query and context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): User context\n",
    "        document (str): Document text\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        float: Relevance score from 0-10\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance considering context\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance considering context.\n",
    "        Rate the document on a scale from 0 to 10 based on how well it addresses the query\n",
    "        when considering the provided context, where:\n",
    "        0 = Completely irrelevant\n",
    "        10 = Perfectly addresses the query in the given context\n",
    "\n",
    "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate document if it's too long\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # User prompt containing the query, context, and document preview\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    Relevance score considering context (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the score from the model's response\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numeric score using regex\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
    "    else:\n",
    "        # Default score if extraction fails\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for contextual queries integrating user context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        user_context (str): Additional user context\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # If no user context provided, try to infer it from the query\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
    "For the given query, infer what contextual information might be relevant or implied \n",
    "but not explicitly stated. Focus on what background would help answering this query.\n",
    "\n",
    "Return a brief description of the implied context.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
    "        \n",
    "        # Generate the inferred context using the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Extract and print the inferred context\n",
    "        user_context = response.choices[0].message.content.strip()\n",
    "        print(f\"Inferred context: {user_context}\")\n",
    "    \n",
    "    # Reformulate the query to incorporate context\n",
    "    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n",
    "    Given a query and some contextual information, create a more specific query that \n",
    "    incorporates the context to get more relevant information.\n",
    "\n",
    "    Return ONLY the reformulated query without explanation.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {user_context}\n",
    "\n",
    "    Reformulate the query to incorporate this context:\"\"\"\n",
    "    \n",
    "    # Generate the contextualized query using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract and print the contextualized query\n",
    "    contextualized_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Contextualized query: {contextualized_query}\")\n",
    "    \n",
    "    # Retrieve documents based on the contextualized query\n",
    "    query_embedding = create_embeddings(contextualized_query)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Rank documents considering both relevance and user context\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    "        # Score document relevance considering the context\n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "    \n",
    "    # Sort by context relevance and return top k results\n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Perform adaptive retrieval by selecting and executing the appropriate strategy.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context for contextual queries\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    # Classify the query to determine its type\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # Select and execute the appropriate retrieval strategy based on the query type\n",
    "    if query_type == \"Factual\":\n",
    "        # Use the factual retrieval strategy for precise information\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "        # Use the analytical retrieval strategy for comprehensive coverage\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "        # Use the opinion retrieval strategy for diverse perspectives\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "        # Use the contextual retrieval strategy, incorporating user context\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "        # Default to factual retrieval strategy if classification fails\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    \n",
    "    return results  # Return the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"AI_Information.pdf\"\n",
    "\n",
    "# Process the document (extract text, create chunks, generate questions, build vector store)\n",
    "vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search using the query and vector store.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query.\n",
    "    vector_store (SimpleVectorStore): The vector store to search in.\n",
    "    k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: Top k most relevant items.\n",
    "    \"\"\"\n",
    "    # Create embedding for the query\n",
    "    query_embedding_response = create_embeddings(query)\n",
    "    query_embedding = query_embedding_response\n",
    "    \n",
    "    # Search the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(query, results, query_type, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generate a response based on query, retrieved documents, and query type.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved documents\n",
    "        query_type (str): Type of query\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Prepare context from retrieved documents by joining their texts with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "    \n",
    "    # Create custom system prompt based on query type\n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n",
    "    Answer the question based on the provided context. Focus on accuracy and precision.\n",
    "    If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n",
    "    Based on the provided context, offer a comprehensive analysis of the topic.\n",
    "    Cover different aspects and perspectives in your explanation.\n",
    "    If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n",
    "    Based on the provided context, present different perspectives on the topic.\n",
    "    Ensure fair representation of diverse opinions without showing bias.\n",
    "    Acknowledge where the context presents limited viewpoints.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n",
    "    Answer the question considering both the query and its context.\n",
    "    Make connections between the query context and the information in the provided documents.\n",
    "    If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n",
    "        \n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n",
    "    \n",
    "    # Create user prompt by combining the context and the query\n",
    "    user_prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a helpful response based on the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with adaptive retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, retrieved documents, query type, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Classify the query to determine its type\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # Retrieve documents using the adaptive retrieval strategy based on the query type\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # Generate a response based on the query, retrieved documents, and query type\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "    \n",
    "    # Compile the results into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the concerns about AI?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 42 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 42 chunks to the vector store\n",
      "Query classified as: Opinion\n",
      "Executing Opinion retrieval strategy for: 'what are the concerns about AI?'\n",
      "Identified viewpoints: ['1. Ethical Concerns: Some people are worried about the ethical implications of AI, including issues related to privacy, bias in decision-making algorithms, and the potential for AI to be used for malicious purposes.', '2. Job Displacement: Another perspective is the concern about the impact of AI on employment, with fears that automation and AI technologies could lead to widespread job displacement and economic disruption.', '3. Safety and Control: There are also concerns about the safety and control of AI systems, including worries about the potential for AI to surpass human intelligence and the need for robust regulations and safeguards to prevent AI from causing harm.']\n"
     ]
    }
   ],
   "source": [
    "chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "# Initialize collection for storing comparison results\n",
    "\n",
    "# Process each test query with both retrieval methods\n",
    "\n",
    "# Create embedding for the query\n",
    "query_embedding = create_embeddings(query)\n",
    "# Retrieve documents using simple vector similarity\n",
    "standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "# Generate response using a generic approach\n",
    "standard_response = generate_response(query, standard_docs, \"General\")\n",
    "\n",
    "# Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\n",
    "query_type = classify_query(query)\n",
    "# Retrieve documents using the strategy appropriate for this query type\n",
    "adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
    "# Generate a response tailored to the query type\n",
    "adaptive_response = generate_response(query, adaptive_docs, query_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context provided, the concerns about AI include transparency and explainability, privacy and security, job displacement, autonomy and control, bias and fairness, and the potential for unintended consequences. These concerns highlight the need to address ethical, societal, and practical challenges associated with the development and deployment of AI.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The concerns about AI raised in the provided context encompass various ethical and societal implications. One concern is the potential for bias and unfair outcomes in AI systems, as they can inherit and amplify biases present in the data they are trained on. This raises the critical challenge of ensuring fairness and mitigating bias in AI systems. Additionally, the lack of transparency and explainability in some AI models, particularly deep learning models, makes it difficult to understand how they arrive at their decisions, which can impact trust and accountability. Privacy and data security are also significant concerns, as AI systems often rely on large amounts of data, raising questions about responsible data handling and protection of sensitive information. Furthermore, the potential for job displacement due to AI-driven automation and questions about autonomy, control, and unintended consequences as AI systems become more autonomous are also important societal concerns. Lastly, the potential weaponization of AI is a concern that needs to be addressed.\\n\\nIt's important to note that while these concerns are valid, there are also efforts to address them. Principles of Ethical AI have been developed to guide the development and deployment of AI systems to ensure fairness, transparency, accountability, and societal benefit. Additionally, there are ongoing efforts to address bias in AI through careful data collection, algorithm design, and ongoing monitoring and evaluation. Transparency and explainability are being enhanced through Explainable AI (XAI) techniques, aiming to make AI decisions more understandable. Furthermore, education and awareness campaigns are being used to inform the public about AI, its impacts, and its potential, aligning with societal values. These efforts aim to mitigate the concerns and ensure that AI development and deployment align with ethical and societal principles.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
