{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\") # Fazer o embedding usando esse\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(text), chunk_size-overlap):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text_chunk, num_questions=5, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generates relevant questions that can be answered from the given text chunk.\n",
    "\n",
    "    Args:\n",
    "    text_chunk (str): The text chunk to generate questions from.\n",
    "    num_questions (int): Number of questions to generate.\n",
    "    model (str): The model to use for question generation.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: List of generated questions.\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's behavior\n",
    "    system_prompt = \"You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.\"\n",
    "    \n",
    "    # Define the user prompt with the text chunk and the number of questions to generate\n",
    "    user_prompt = f\"\"\"\n",
    "    Based on the following text, generate {num_questions} different questions that can be answered using only this text:\n",
    "\n",
    "    {text_chunk}\n",
    "    \n",
    "    Format your response as a numbered list of questions only, with no additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate questions using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract and clean questions from the response\n",
    "    questions_text = response.choices[0].message.content.strip()\n",
    "    questions = []\n",
    "    \n",
    "    # Extract questions using regex pattern matching\n",
    "    for line in questions_text.split('\\n'):\n",
    "        # Remove numbering and clean up whitespace\n",
    "        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "        if cleaned_line and cleaned_line.endswith('?'):\n",
    "            questions.append(cleaned_line)\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    response = embedder.encode(text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A lightweight vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension=1536):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \n",
    "        Args:\n",
    "            dimension (int): Dimension of embeddings\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vectors = []\n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_documents(self, documents, vectors=None, metadata=None):\n",
    "        \"\"\"\n",
    "        Add documents to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            documents (List[str]): List of document chunks\n",
    "            vectors (List[List[float]], optional): List of embedding vectors\n",
    "            metadata (List[Dict], optional): List of metadata dictionaries\n",
    "        \"\"\"\n",
    "        if vectors is None:\n",
    "            vectors = [None] * len(documents)\n",
    "        \n",
    "        if metadata is None:\n",
    "            metadata = [{} for _ in range(len(documents))]\n",
    "        \n",
    "        for doc, vec, meta in zip(documents, vectors, metadata):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(vec)\n",
    "            self.metadata.append(meta)\n",
    "    \n",
    "    def search(self, query_vector, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for most similar documents.\n",
    "        \n",
    "        Args:\n",
    "            query_vector (List[float]): Query embedding vector\n",
    "            top_k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of results with documents, scores, and metadata\n",
    "        \"\"\"\n",
    "        if not self.vectors or not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # Convert query vector to numpy array\n",
    "        query_array = np.array(query_vector)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            if vector is not None:\n",
    "                # Compute cosine similarity\n",
    "                similarity = np.dot(query_array, vector) / (\n",
    "                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n",
    "                )\n",
    "                similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top-k results\n",
    "        results = []\n",
    "        for i, score in similarities[:top_k]:\n",
    "            results.append({\n",
    "                \"document\": self.documents[i],\n",
    "                \"score\": float(score),\n",
    "                \"metadata\": self.metadata[i]\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=800):\n",
    "    \"\"\"\n",
    "    Process a document for use with RSE.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore, Dict]: Chunks, vector store, and document info\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the extracted text into non-overlapping segments\n",
    "    chunks = chunk_text(text, chunk_size=chunk_size, overlap=0)\n",
    "    \n",
    "    # Generate embeddings for the text chunks\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Create an instance of the SimpleVectorStore\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add documents with metadata (including chunk index for later reconstruction)\n",
    "    metadata = [{\"chunk_index\": i, \"source\": pdf_path} for i in range(len(chunks))]\n",
    "    vector_store.add_documents(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "    # Track original document structure for segment reconstruction\n",
    "    doc_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"source\": pdf_path,\n",
    "    }\n",
    "    \n",
    "    return chunks, vector_store, doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    Calculate chunk values by combining relevance and position.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query text\n",
    "        chunks (List[str]): List of document chunks\n",
    "        vector_store (SimpleVectorStore): Vector store containing the chunks\n",
    "        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: List of chunk values\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # Get all chunks with similarity scores\n",
    "    num_chunks = len(chunks)\n",
    "    results = vector_store.search(query_embedding, top_k=num_chunks)\n",
    "    \n",
    "    # Create a mapping of chunk_index to relevance score\n",
    "    relevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n",
    "    \n",
    "    # Calculate chunk values (relevance score minus penalty)\n",
    "    chunk_values = []\n",
    "    for i in range(num_chunks):\n",
    "        # Get relevance score or default to 0 if not in results\n",
    "        score = relevance_scores.get(i, 0.0)\n",
    "        # Apply penalty to convert to a value where irrelevant chunks have negative value\n",
    "        value = score - irrelevant_chunk_penalty\n",
    "        chunk_values.append(value)\n",
    "    \n",
    "    return chunk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
    "    \"\"\"\n",
    "    Find the best segments using a variant of the maximum sum subarray algorithm.\n",
    "    \n",
    "    Args:\n",
    "        chunk_values (List[float]): Values for each chunk\n",
    "        max_segment_length (int): Maximum length of a single segment\n",
    "        total_max_length (int): Maximum total length across all segments\n",
    "        min_segment_value (float): Minimum value for a segment to be considered\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: List of (start, end) indices for best segments\n",
    "    \"\"\"\n",
    "    \n",
    "    best_segments = []\n",
    "    segment_scores = []\n",
    "    total_included_chunks = 0\n",
    "    \n",
    "    # Keep finding segments until we hit our limits\n",
    "    while total_included_chunks < total_max_length:\n",
    "        best_score = min_segment_value  # Minimum threshold for a segment\n",
    "        best_segment = None\n",
    "        \n",
    "        # Try each possible starting position\n",
    "        for start in range(len(chunk_values)):\n",
    "            # Skip if this start position is already in a selected segment\n",
    "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
    "                continue\n",
    "                \n",
    "            # Try each possible segment length\n",
    "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
    "                end = start + length\n",
    "                \n",
    "                # Skip if end position is already in a selected segment\n",
    "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate segment value as sum of chunk values\n",
    "                segment_value = sum(chunk_values[start:end])\n",
    "                \n",
    "                # Update best segment if this one is better\n",
    "                if segment_value > best_score:\n",
    "                    best_score = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # If we found a good segment, add it\n",
    "        if best_segment:\n",
    "            best_segments.append(best_segment)\n",
    "            segment_scores.append(best_score)\n",
    "            total_included_chunks += best_segment[1] - best_segment[0]\n",
    "        else:\n",
    "            # No more good segments to find\n",
    "            break\n",
    "    \n",
    "    # Sort segments by their starting position for readability\n",
    "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
    "    \n",
    "    return best_segments, segment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(chunks, best_segments):\n",
    "    \"\"\"\n",
    "    Reconstruct text segments based on chunk indices.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): List of all document chunks\n",
    "        best_segments (List[Tuple[int, int]]): List of (start, end) indices for segments\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of reconstructed text segments\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []  # Initialize an empty list to store the reconstructed segments\n",
    "    \n",
    "    for start, end in best_segments:\n",
    "        # Join the chunks in this segment to form the complete segment text\n",
    "        segment_text = \" \".join(chunks[start:end])\n",
    "        # Append the segment text and its range to the reconstructed_segments list\n",
    "        reconstructed_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"segment_range\": (start, end),\n",
    "        })\n",
    "    \n",
    "    return reconstructed_segments  # Return the list of reconstructed text segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segments_for_context(segments):\n",
    "    \"\"\"\n",
    "    Format segments into a context string for the LLM.\n",
    "    \n",
    "    Args:\n",
    "        segments (List[Dict]): List of segment dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted context text\n",
    "    \"\"\"\n",
    "    context = []  # Initialize an empty list to store the formatted context\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # Create a header for each segment with its index and chunk range\n",
    "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
    "        context.append(segment_header)  # Add the segment header to the context list\n",
    "        context.append(segment['text'])  # Add the segment text to the context list\n",
    "        context.append(\"-\" * 80)  # Add a separator line for readability\n",
    "    \n",
    "    # Join all elements in the context list with double newlines and return the result\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"AI_Information.pdf\"\n",
    "\n",
    "# Process the document (extract text, create chunks, generate questions, build vector store)\n",
    "vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search using the query and vector store.\n",
    "\n",
    "    Args:\n",
    "    query (str): The search query.\n",
    "    vector_store (SimpleVectorStore): The vector store to search in.\n",
    "    k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "    List[Dict]: Top k most relevant items.\n",
    "    \"\"\"\n",
    "    # Create embedding for the query\n",
    "    query_embedding_response = create_embeddings(query)\n",
    "    query_embedding = query_embedding_response\n",
    "    \n",
    "    # Search the vector store\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"gpt-3.5-turbo-1106\"):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query and context.\n",
    "\n",
    "    Args:\n",
    "    query (str): User's question.\n",
    "    context (str): Context information retrieved from the vector store.\n",
    "    model (str): Model to use for response generation.\n",
    "\n",
    "    Returns:\n",
    "    str: Generated response.\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with Relevant Segment Extraction.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the document\n",
    "        query (str): User query\n",
    "        chunk_size (int): Size of chunks\n",
    "        irrelevant_chunk_penalty (float): Penalty for irrelevant chunks\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Result with query, segments, and response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    # Calculate relevance scores and chunk values based on the query\n",
    "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
    "    \n",
    "    # Find the best segments of text based on chunk values\n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values, \n",
    "        max_segment_length=20, \n",
    "        total_max_length=30, \n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "    \n",
    "    # Reconstruct text segments from the best chunks\n",
    "    segments = reconstruct_segments(chunks, best_segments)\n",
    "    \n",
    "    # Format the segments into a context string for the language model\n",
    "    context = format_segments_for_context(segments)\n",
    "    \n",
    "    # Generate a response from the language model using the context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Compile the result into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"segments\": segments,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the SentenceTransformer model and set the similarity function\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "model.similarity_fn_name = SimilarityFunction.DOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_validation_data(irrelevant_chunk_penalty):\n",
    "    system_prompt = \"\"\"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly and exactly from the provided context, respond with: 'I do not have enough information to answer that.'\n",
    "    First think about the keywords from the question and then use them to elaborate the answer.\n",
    "    The response needs to be just the answer sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load the validation data from the JSON file\n",
    "    with open('val.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # List to store the results for each sample\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each example in the validation data\n",
    "    for idx, item in enumerate(data):\n",
    "        query = item['question']\n",
    "        ideal_answer = item['ideal_answer']\n",
    "        \n",
    "        # Retrieve the top k most relevant context chunks\n",
    "                \n",
    "        # Generate the AI response using the system prompt and the user prompt\n",
    "        result = rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=irrelevant_chunk_penalty)\n",
    "        ai_response = result['response']\n",
    "        # Evaluate similarity using SentenceTransformer\n",
    "        # Encode the AI response and ideal answer\n",
    "        embedding_response = model.encode([ai_response])\n",
    "        embedding_ideal = model.encode([ideal_answer])\n",
    "        # Compute similarity score (result is a 1x1 matrix; extract the single value)\n",
    "        similarity_matrix = model.similarity(embedding_response, embedding_ideal)\n",
    "        score = similarity_matrix[0][0].numpy()\n",
    "        \n",
    "        # Prepare the result dictionary with dynamic context columns\n",
    "        result = {\n",
    "            \"Query\": query,\n",
    "            \"Ideal Answer\": ideal_answer,\n",
    "            \"AI Response\": ai_response,\n",
    "            \"Score\": score\n",
    "        }\n",
    "        \n",
    "        # Append the result to the list\n",
    "        results.append(result)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Query",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ideal Answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "AI Response",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Score",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "171d853f-b53c-41f8-8d0a-09cbca0245f0",
       "rows": [
        [
         "0",
         "What is 'Explainable AI' and why is it considered important?",
         "Explainable AI (XAI) aims to make AI systems more transparent and understandable, providing insights into how they make decisions. It's considered important for building trust, accountability, and ensuring fairness in AI systems.",
         "Explainable AI (XAI) aims to make AI systems more transparent and understandable. It focuses on developing methods for explaining AI decisions, enhancing trust, and improving accountability. It is considered important to address concerns about the opacity of AI systems, enabling users to assess their fairness, accuracy, and potential impacts.",
         "0.96003276"
        ],
        [
         "1",
         "Can AI be used to predict earthquakes?",
         "I don't have enough information to answer that.",
         "I do not have enough information to answer that.",
         "0.9543278"
        ],
        [
         "2",
         "What are some of the ethical concerns related to AI-powered facial recognition?",
         "I don't have enough information to answer that.",
         "Bias and fairness, transparency and explainability, privacy and security, and autonomy and control are some of the ethical concerns related to AI-powered facial recognition.",
         "0.19976649"
        ],
        [
         "3",
         "How does AI contribute to personalized medicine?",
         "AI enables personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions to specific needs. This enhances treatment effectiveness and reduces adverse effects.",
         "AI contributes to personalized medicine by analyzing individual patient data, predicting treatment responses, and tailoring interventions. This enhances treatment effectiveness and reduces adverse effects.",
         "0.98355246"
        ],
        [
         "4",
         "Does the document mention any specific companies developing AI technology?",
         "I don't have enough information to answer that.",
         "The document does mention specific companies developing AI technology, such as Siri and Alexa for virtual assistants, as well as companies involved in AI applications like self-driving cars, medical diagnostics, and financial modeling.",
         "0.10744828"
        ],
        [
         "5",
         "What is the role of AI in smart grids?",
         "AI optimizes energy distribution in smart grids by enabling real-time monitoring, demand response, and integration of distributed energy resources. This enhances grid reliability, reduces energy waste, and supports renewable energy.",
         "AI optimizes energy management in smart cities by predicting demand, managing supply, and promoting energy efficiency. AI-powered systems enhance grid stability, reduce energy waste, and support the integration of renewable energy sources.",
         "0.7971191"
        ],
        [
         "6",
         "Can AI write a complete, original novel?",
         "I don't have enough information to answer that.",
         "Yes, AI can write a complete, original novel using AI-powered writing tools that can generate content and assist in the creative writing process.",
         "0.049507573"
        ],
        [
         "7",
         "What is a 'cobot'?",
         "It mentions collaborative settings (cobots) in industrial robots.",
         "A 'cobot' is a collaborative robot that works alongside humans in manufacturing settings.",
         "0.75007105"
        ],
        [
         "8",
         "What is Direct Air Capture (DAC) used for?",
         "DAC technology removes CO2 directly from the atmosphere. The captured CO2 can be stored or used in various applications.",
         "I do not have enough information to answer that.",
         "0.04811383"
        ],
        [
         "9",
         "Is AI currently being used to control nuclear weapons systems?",
         "I don't have enough information to answer that.",
         "I do not have enough information to answer that.",
         "0.9543278"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Ideal Answer</th>\n",
       "      <th>AI Response</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is 'Explainable AI' and why is it conside...</td>\n",
       "      <td>Explainable AI (XAI) aims to make AI systems m...</td>\n",
       "      <td>Explainable AI (XAI) aims to make AI systems m...</td>\n",
       "      <td>0.96003276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can AI be used to predict earthquakes?</td>\n",
       "      <td>I don't have enough information to answer that.</td>\n",
       "      <td>I do not have enough information to answer that.</td>\n",
       "      <td>0.9543278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some of the ethical concerns related ...</td>\n",
       "      <td>I don't have enough information to answer that.</td>\n",
       "      <td>Bias and fairness, transparency and explainabi...</td>\n",
       "      <td>0.19976649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does AI contribute to personalized medicine?</td>\n",
       "      <td>AI enables personalized medicine by analyzing ...</td>\n",
       "      <td>AI contributes to personalized medicine by ana...</td>\n",
       "      <td>0.98355246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does the document mention any specific compani...</td>\n",
       "      <td>I don't have enough information to answer that.</td>\n",
       "      <td>The document does mention specific companies d...</td>\n",
       "      <td>0.10744828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the role of AI in smart grids?</td>\n",
       "      <td>AI optimizes energy distribution in smart grid...</td>\n",
       "      <td>AI optimizes energy management in smart cities...</td>\n",
       "      <td>0.7971191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Can AI write a complete, original novel?</td>\n",
       "      <td>I don't have enough information to answer that.</td>\n",
       "      <td>Yes, AI can write a complete, original novel u...</td>\n",
       "      <td>0.049507573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is a 'cobot'?</td>\n",
       "      <td>It mentions collaborative settings (cobots) in...</td>\n",
       "      <td>A 'cobot' is a collaborative robot that works ...</td>\n",
       "      <td>0.75007105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is Direct Air Capture (DAC) used for?</td>\n",
       "      <td>DAC technology removes CO2 directly from the a...</td>\n",
       "      <td>I do not have enough information to answer that.</td>\n",
       "      <td>0.04811383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is AI currently being used to control nuclear ...</td>\n",
       "      <td>I don't have enough information to answer that.</td>\n",
       "      <td>I do not have enough information to answer that.</td>\n",
       "      <td>0.9543278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0  What is 'Explainable AI' and why is it conside...   \n",
       "1             Can AI be used to predict earthquakes?   \n",
       "2  What are some of the ethical concerns related ...   \n",
       "3   How does AI contribute to personalized medicine?   \n",
       "4  Does the document mention any specific compani...   \n",
       "5             What is the role of AI in smart grids?   \n",
       "6           Can AI write a complete, original novel?   \n",
       "7                                 What is a 'cobot'?   \n",
       "8         What is Direct Air Capture (DAC) used for?   \n",
       "9  Is AI currently being used to control nuclear ...   \n",
       "\n",
       "                                        Ideal Answer  \\\n",
       "0  Explainable AI (XAI) aims to make AI systems m...   \n",
       "1    I don't have enough information to answer that.   \n",
       "2    I don't have enough information to answer that.   \n",
       "3  AI enables personalized medicine by analyzing ...   \n",
       "4    I don't have enough information to answer that.   \n",
       "5  AI optimizes energy distribution in smart grid...   \n",
       "6    I don't have enough information to answer that.   \n",
       "7  It mentions collaborative settings (cobots) in...   \n",
       "8  DAC technology removes CO2 directly from the a...   \n",
       "9    I don't have enough information to answer that.   \n",
       "\n",
       "                                         AI Response        Score  \n",
       "0  Explainable AI (XAI) aims to make AI systems m...   0.96003276  \n",
       "1   I do not have enough information to answer that.    0.9543278  \n",
       "2  Bias and fairness, transparency and explainabi...   0.19976649  \n",
       "3  AI contributes to personalized medicine by ana...   0.98355246  \n",
       "4  The document does mention specific companies d...   0.10744828  \n",
       "5  AI optimizes energy management in smart cities...    0.7971191  \n",
       "6  Yes, AI can write a complete, original novel u...  0.049507573  \n",
       "7  A 'cobot' is a collaborative robot that works ...   0.75007105  \n",
       "8   I do not have enough information to answer that.   0.04811383  \n",
       "9   I do not have enough information to answer that.    0.9543278  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await process_validation_data(irrelevant_chunk_penalty= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6737001419067383\n",
      "0.600471019744873\n",
      "0.5185361862182617\n",
      "0.7674449920654297\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 9):\n",
    "    result_df = await process_validation_data(irrelevant_chunk_penalty= k/10)\n",
    "    print(result_df['Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
